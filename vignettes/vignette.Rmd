---
title: "Gaussian Process Gibbs Sampler"
author: Andrew Raim
date: 2023-05-08
output:
  rmarkdown::html_vignette:
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Gaussian Process Gibbs Sampler}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
	collapse = TRUE,
	comment = "#>",
	fig.show = "hold"
)
```

\renewcommand{\vec}[1]{\boldsymbol{#1}}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Diag}{Diag}
\newcommand{\iid}{\stackrel{\text{iid}}{\sim}}
\newcommand{\indep}{\stackrel{\text{ind}}{\sim}}

# Introduction

In this package, we fit a Gaussian Process model using Gibbs sampling. First
let us write the model. Let $\mu(\cdot) : \mathbb{R}^d \rightarrow \mathbb{R}$
be a function that we would like to perform inference on. We may not know
anything about it's form, but have observed data 
$(\vec{x}_i, y_i)$, $i = 1, \ldots, n$, where $y_i$ is a noisy observation of
$\mu(\vec{x}_i)$. Suppose $k(\cdot, \cdot)$ is a covariance kernel so that 
$k(\vec{x}, \vec{x}') = \Cov(\mu(\vec{x}), \mu(\vec{x}'))$, but where any
parameters are specified. An example is the squared exponential kernel
$k(\vec{x}, \vec{x}') = \exp\{ -\frac{1}{2} \lVert \vec{x} - \vec{x}' \rVert^2 \}$.

We assume the model

$$
\begin{align*}
&y_i = f(\vec{x}_i) + \epsilon_i, \quad \epsilon_i \sim \text{N}(0, \sigma^2), \quad
i = 1, \ldots, n, \\
&\mu \sim \text{GP}(0, k(\cdot, \cdot)), \quad
\sigma^2 \sim \text{IG}(a_\sigma, b_\sigma).
\end{align*}
$$

With matrix/vector notation, we write the model as

$$
\begin{align*}
&\vec{y} = \mu(\vec{X}) + \vec{\epsilon}, \quad
\vec{\epsilon} \sim \text{N}(\vec{0}, \sigma^2 \vec{I}), \\
&\mu(\vec{X}) \sim \text{N}(\vec{0}, k(\vec{X}, \vec{X})), \\
&\sigma^2 \sim \text{IG}(a_\sigma, b_\sigma).
\end{align*}
$$

where $k(\vec{X}, \vec{X})$ is an $n \times n$ matrix with $(i,j)$th element
$k(\vec{x}_i, \vec{x}_j)$,
$\mu(\vec{X}) = \mu(\vec{x}_1, \ldots, \vec{x}_n)$,
$\vec{y} = (y_1, \ldots, y_n)$, and
$\vec{\epsilon} = (\epsilon_1, \ldots, \epsilon_n)$.

# Sampling from Posterior

We will use a Gibbs sampler which repeats the following two steps for $R$
iterations:

1. Sample $\vec{\mu}$ from $[\mu(\vec{X}) \mid \sigma^2, \vec{y}]$,
2. Sample $\sigma^2$ from $[\sigma^2 \mid \mu(\vec{X}), \vec{y}]$.

This yields draws $(\sigma^{2(r)}, \vec{\mu}^{(r)})$ for $r = 1, \ldots, R$.
Both conditionals have familiar forms. To find them, first let us write the
joint distribution of all random variables in the model
$[\vec{y}, \mu(\vec{X}), \sigma^2]$:

$$
\begin{align*}
\pi(\vec{y}, \mu(\vec{X}), \sigma^2) =
\text{N}(\vec{y} \mid \mu(\vec{X}), \sigma^2 \vec{I}) \cdot
\text{N}(\mu(\vec{X}) \mid \vec{0}, k(\vec{X}, \vec{X})) \cdot
\text{IG}(\sigma^2 \mid a_\sigma, b_\sigma).
\end{align*}
$$

The conditional distribution $[\sigma^2 \mid \vec{y}, \mu(\vec{X})]$ is then

$$
\begin{align*}
\pi(\sigma^2 \mid \vec{y}, \mu(\vec{X}))
%
&= \text{N}(\vec{y} \mid \mu(\vec{X}), \sigma^2 \vec{I}) \cdot
\text{IG}(\sigma^2 \mid a_\sigma, b_\sigma) \\
%
&= (\sigma^2)^{-n/2} \exp\left\{ -\frac{1}{2 \sigma^2} \lVert \vec{y} - \mu(\vec{X}) \rVert^2 \right\}
(\sigma^2)^{-a_\sigma - 1} \exp\{ -b_\sigma / \sigma^2 \} \\
%
&= (\sigma^2)^{-(a_\sigma + n/2) - 1} \exp\left\{
-\frac{1}{\sigma^2} \left[ b_\sigma + \frac{1}{2} \lVert \vec{y} - \mu(\vec{X}) \rVert^2 \right]
\right\} \\
%
&= \text{IG}(a_\sigma^*, b_\sigma^*),
\end{align*}
$$

where $a_\sigma^* = a_\sigma + n/2$ and
$b_\sigma^* = b_\sigma + \frac{1}{2} \lVert \vec{y} - \mu(\vec{X}) \rVert^2$.


The conditional distribution $[\mu(\vec{X}) \mid \vec{y}, \sigma^2]$ is

$$
\begin{align*}
&\pi(\mu(\vec{X}) \mid \vec{y}, \sigma^2) \\
%
&\quad= \text{N}(\vec{y} \mid \mu(\vec{X}), \sigma^2 \vec{I}) \cdot
\text{N}(\mu(\vec{X}) \mid \vec{0}, k(\vec{X}, \vec{X})) \\
%
&\quad= \exp\left\{
-\frac{1}{2 \sigma^2} [\vec{y} - \mu(\vec{X})]^\top [\vec{y} - \mu(\vec{X})]
\right\}
\exp\left\{
-\frac{1}{2} \mu(\vec{X})^\top k(\vec{X}, \vec{X})^{-1} \mu(\vec{X})
\right\} \\
%
&\quad\propto \exp\left\{
-\frac{1}{2} \left[
\mu(\vec{X})^\top 
\underbrace{\left[ k(\vec{X}, \vec{X})^{-1} + \sigma^{-2} \vec{I} \right]}_{\vec{\Omega}}
\mu(\vec{X})
- 2 \sigma^{-2} \vec{y}^\top \mu(\vec{X}) 
\right]
\right\} \\
%
&\quad\propto \exp\left\{
-\frac{1}{2} \left[
\mu(\vec{X})^\top \vec{\Omega} \mu(\vec{X})
- 2 \underbrace{\sigma^{-2} \vec{y}^\top \vec{\Omega}^{-1}}_{\vec{\vartheta}^\top} \vec{\Omega} \mu(\vec{X}) 
\right]
\right\} \\
%
&\quad\propto \exp\left\{
-\frac{1}{2} 
[\mu(\vec{X}) - \vec{\vartheta}]^\top \vec{\Omega} [\mu(\vec{X}) - \vec{\vartheta}]
\right\} \\
%
&\quad\propto \text{N}(\mu(\vec{X}) \mid \vec{\vartheta}, \vec{\Omega}^{-1}).
\end{align*}
$$


# Posterior Predictive Distribution

Suppose there are $n_0$ (potentially new) inputs
$\vec{X}_0 = (\vec{x}_{01} \; \cdots \; \vec{x}_{0 n_0})^\top$ for which we
would like predictions. We can sample from the posterior predictive
distribution by drawing $f(\vec{X}_0)$ from
$[f(\vec{X}_0) \mid \vec{y}, \sigma^2]$ for each $\sigma^2 = \sigma^{2(r)}$.

Starting from the distribution $[\vec{y}, f(\vec{X}_0) \mid \sigma^2]$ 

$$
\begin{bmatrix}
\vec{y} \\
f(\vec{X}_0)
\end{bmatrix}
\sim \text{N}\left(
\begin{bmatrix}
\vec{0} \\
\vec{0}
\end{bmatrix},
\begin{bmatrix}
k(\vec{X}, \vec{X}) + \sigma^2 \vec{I}   & k(\vec{X}, \vec{X}_0) \\
k(\vec{X}_0, \vec{X})                    & k(\vec{X}_0, \vec{X}_0)
\end{bmatrix}
\right),
$$

The distribution $[f(\vec{X}_0) \mid \vec{y}, \sigma^2]$ is determined to be
$\text{N}(\vec{\mu}_0, \vec{\Sigma}_0)$ with

$$
\begin{align*}
&\vec{\mu}_0 = k(\vec{X}_0, \vec{X}) [k(\vec{X},\vec{X}) + \sigma^2 \vec{I}]^{-1} \vec{y}, \\
&\vec{\Sigma}_0 = k(\vec{X}_0, \vec{X}_0) - k(\vec{X}_0,\vec{X}) [k(\vec{X}, \vec{X}) + \sigma^2 \vec{I}]^{-1} k(\vec{X}, \vec{X}_0).
\end{align*}
$$

# Computation of Matrices

We can simplify some of the computations using the spectral decomposition of
$k(\vec{X},\vec{X})$ be $\vec{U} \vec{\Lambda} \vec{U}^\top$ and suppose
$\vec{W} \sim \text{N}(\vec{\vartheta}, \vec{\Omega}^{-1})$.
Then

$$
\vec{U}^\top \vec{W} \sim \text{N}(\vec{U}^\top \vec{\vartheta}, \vec{U}^\top \vec{\Omega}^{-1} \vec{U}).
$$

We have 

$$
\begin{align*}
\vec{U}^\top \vec{\Omega}^{-1} \vec{U}
&= \vec{U}^\top [k(\vec{X}, \vec{X})^{-1} + \sigma^{-2} \vec{I}]^{-1} \vec{U} \\
&= \{ \vec{U}^\top [ k(\vec{X}, \vec{X})^{-1} + \sigma^{-2} \vec{I} ] \vec{U} \}^{-1} \\
&= [ \vec{\Lambda}^{-1} + \sigma^{-2} \vec{I} ]^{-1} \\
&= \Diag\left(\frac{1}{\lambda_1^{-1} + \sigma^{-2}}, \ldots, \frac{1}{\lambda_n^{-1} + \sigma^{-2}} \right)
\end{align*}
$$

so that the components of $\vec{W}$ are independent. Furthermore,

$$
\begin{align*}
\vec{U}^\top \vec{\vartheta}
&= \sigma^{-2} \vec{U}^\top \vec{\Omega}^{-1} \vec{y} \\
&= \sigma^{-2} \vec{U}^\top [k(\vec{X}, \vec{X})^{-1} + \sigma^{-2} \vec{I}]^{-1} \vec{y} \\
&= \sigma^{-2} \vec{U}^\top [\vec{U} \vec{\Lambda}^{-1} \vec{U}^\top + \sigma^{-2} \vec{I}]^{-1} \vec{y} \\
&= \sigma^{-2} [\vec{U} \vec{\Lambda}^{-1} + \sigma^{-2} \vec{U}]^{-1} \vec{y} \\
&= \sigma^{-2} [\vec{\Lambda}^{-1} + \sigma^{-2} \vec{I}]^{-1} \underbrace{\vec{U}^\top \vec{y}}_{\vec{z}} \\
&= \sigma^{-2}
\Diag\left(\frac{1}{\lambda_1^{-1} + \sigma^{-2}}, \ldots, \frac{1}{\lambda_n^{-1} + \sigma^{-2}} \right)
\vec{z} \\
&= 
\Diag\left(\frac{z_1}{1 + \sigma^2 / \lambda_1}, \ldots,
\frac{z_n}{1 + \sigma^2 / \lambda_n} \right) \\
\end{align*}
$$

Therefore, we can sample the components of $\vec{W}$ independently as

$$W_i \sim \text{N}\left(
\frac{z_i}{1 + \sigma^2 / \lambda_i}, \frac{1}{\lambda_n^{-1} + \sigma^{-2}}
\right),
$$

then transform to $\vec{\mu} = \vec{U} \vec{W}$ to get a desired draw from
$[\mu(\vec{X}) \mid \vec{y}, \sigma^2]$.

We can also simplify computations involving the posterior predictive
distribution. Let $\vec{K}_{21} = k(\vec{X}_0, \vec{X})$ and
$\vec{K}_{22} = k(\vec{X}_0, \vec{X}_0)$. We have

$$
\begin{align*}
\vec{\mu}_0
&= \vec{K}_{21} [\vec{K}_{11} + \sigma^2 \vec{I}]^{-1} \vec{y} \\
&= \vec{K}_{21} [\vec{U} \vec{\Lambda} \vec{U}^\top + \sigma^2 \vec{I}]^{-1} \vec{y} \\
&= \vec{K}_{21} \vec{U} [\vec{\Lambda} + \sigma^2 \vec{I}]^{-1} \vec{U}^\top \vec{y} \\
&= \vec{K}_{21} \vec{U} [\vec{\Lambda} + \sigma^2 \vec{I}]^{-1} \vec{z} \\
&= \vec{K}_{21} \vec{U} \left[\frac{z_1}{\sigma^2 + \lambda_1} \; \cdots \;
\frac{z_n}{\sigma^2 + \lambda_n} \right]^\top
\end{align*}
$$

and

$$
\begin{align*}
\vec{\Sigma}_0
&= \vec{K}_{22} - \vec{K}_{21} [\vec{K}_{11} + \sigma^2 \vec{I}]^{-1} \vec{K}_{12} \\
&= \vec{K}_{22} - \vec{K}_{21} \vec{U} [\vec{\Lambda} + \sigma^2 \vec{I}]^{-1} \vec{U}^\top \vec{K}_{12} \\
&= \vec{K}_{22} - \vec{K}_{21} \vec{U} \Diag\left(\frac{1}{\lambda_1 + \sigma^2}, \ldots, \frac{1}{\lambda_n + \sigma^2} \right) \vec{U}^\top \vec{K}_{12}
\end{align*}
$$

where $\vec{C} = \vec{K}_{21} \vec{U}$. When our interest is in a marginal from
the posterior predictive distribution for input $\vec{x}_0$, these expressions
become

$$
\begin{align*}
&\mu_0
= k(\vec{x}_0, \vec{X}) \vec{U} \left[\frac{z_1}{\sigma^2 + \lambda_1} \; \cdots \;
\frac{z_n}{\sigma^2 + \lambda_n} \right]^\top, \\
%
&\sigma_0^2
= k(\vec{x}_0, \vec{x}_0) - k(\vec{x}_0, \vec{X}) \vec{U} \Diag\left(\frac{1}{\lambda_1 + \sigma^2}, \ldots, \frac{1}{\lambda_n + \sigma^2} \right) \vec{U}^\top k(\vec{X}, \vec{x}_0).
\end{align*}
$$

# Example

Here is a brief example with $\mu(x) = \sin x$. First load necessary packages
and set a seed for reproducibility.

```{r}
library(gputil)
library(ggplot2)
set.seed(1235)
```

Prepare a simulated dataset.

```{r}
mu_true = sin
sigma2_true = 0.05^2
n = 50
x = seq(-4*pi, 4*pi, length.out = n)
X = matrix(x)
muX_true = mu_true(X)
y = rnorm(n, muX_true, sqrt(sigma2_true))
```

Plot the dataset.

```{r, fig.width=7, fig.height=4, out.width="100%"}
dat_plot = data.frame(x = as.numeric(X), y = y)
ggplot() +
	geom_point(data = dat_plot, aes(x, y)) +
	stat_function(fun = mu_true, n = 201, lty = 1) +
	ylab("mu(x)") +
	theme_minimal()
```

Define the squared exponential kernel function.

```{r}
covkern = function(X1, X2) {
	exp(-plgp::distance(X1, X2))
}
```

Run the Gibbs sampler. Note that the main inputs to the sampler are the
observations `y` and the matrix `K` with the covariance kernel evaluated on the
input data.

```{r}
init = get_init(n, muX = muX_true)
fixed = get_fixed(muX = FALSE)
prior = get_prior(a_sigma = 5, b_sigma = 10)
control = get_control(R = 5000, report_period = 1000, save_latent = FALSE)

K = covkern(X, X)
K_eig = eigen(K)

gibbs_out = gibbs(y, K_eig, init, prior, control, fixed)
```

Here is a summary of results from the sampler.

```{r}
print(gibbs_out)
```

Gather the draws of $\sigma^2$ and plot them.

```{r, fig.width=7, fig.height=4, out.width="100%"}
sigma2_mcmc = gibbs_out$sigma2_hist
R = length(sigma2_mcmc)

dat_plot = data.frame(iteration = seq_len(R), sigma2 = sigma2_mcmc)
ggplot(dat_plot) +
	geom_line(aes(x = iteration, y = sigma2)) +
	theme_minimal()
ggplot(dat_plot) +
	geom_histogram(aes(x = sigma2), col = "black", bins = 30) +
	theme_minimal()
```

Use the posterior draws to generate from the posterior predictive distribution.
Our new inputs $\vec{X}_0$ are based on a finer grid on the input space.

```{r}
n0 = 200
x0 = seq(-4*pi, 4*pi, length.out = n0)
X0 = matrix(x0)

# Draw from the marginal posterior predictive distribution for each input
mu0_mcmc = matrix(NA, R, n0)
for (i in 1:n0) {
	if (i %% 50 == 0) {
		raim::logger("Predicting input %d of %d\n", i, n0)
	}
	K22 = covkern(X0[i,], X0[i,])
	K21 = covkern(X0[i,], X)
	mu0_mcmc[,i] = predict(gibbs_out, K22, K21)
}

# Summarize with point estimates and interval bounds
mu0_hat = apply(mu0_mcmc, 2, mean)
mu0_lo = apply(mu0_mcmc, 2, quantile, prob = 0.025)
mu0_hi = apply(mu0_mcmc, 2, quantile, prob = 0.975)
```

Plot predictions and associated level 95% interval, along with true $\mu(x)$
function and noisy data.

```{r, fig.width=7, fig.height=4, out.width="100%"}
dat_plot = data.frame(x = as.numeric(X), y = y)
dat0_plot = data.frame(x0 = x0, mu0_hat = mu0_hat, mu0_lo = mu0_lo, mu0_hi = mu0_hi)

ggplot() +
	geom_point(data = dat_plot, aes(x, y)) +
	stat_function(fun = mu_true, n = 201, lty = 1) +
	geom_line(data = dat0_plot, aes(x = x0, y = mu0_hat), col = "blue", linewidth = 0.8) +
	geom_ribbon(data = dat0_plot,
		aes(x = x0, y = mu0_hat, ymin = mu0_lo, ymax = mu0_hi),
		fill = "blue", alpha = 0.25) +
	ylab("mu(x)") +
	theme_minimal()
```
